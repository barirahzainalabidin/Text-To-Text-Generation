{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5094410,"sourceType":"datasetVersion","datasetId":2958426}],"dockerImageVersionId":30700,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-24T22:48:26.231437Z","iopub.execute_input":"2024-04-24T22:48:26.231812Z","iopub.status.idle":"2024-04-24T22:48:27.541681Z","shell.execute_reply.started":"2024-04-24T22:48:26.231705Z","shell.execute_reply":"2024-04-24T22:48:27.540896Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/3k-conversations-dataset-for-chatbot/Conversation.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# I using text to text generation to complete this project","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-24T22:48:27.543080Z","iopub.execute_input":"2024-04-24T22:48:27.543595Z","iopub.status.idle":"2024-04-24T22:48:27.546835Z","shell.execute_reply.started":"2024-04-24T22:48:27.543563Z","shell.execute_reply":"2024-04-24T22:48:27.546227Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/3k-conversations-dataset-for-chatbot/Conversation.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T22:48:27.547605Z","iopub.execute_input":"2024-04-24T22:48:27.547861Z","iopub.status.idle":"2024-04-24T22:48:27.582961Z","shell.execute_reply.started":"2024-04-24T22:48:27.547837Z","shell.execute_reply":"2024-04-24T22:48:27.582292Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T22:48:27.584090Z","iopub.execute_input":"2024-04-24T22:48:27.584388Z","iopub.status.idle":"2024-04-24T22:48:27.595891Z","shell.execute_reply.started":"2024-04-24T22:48:27.584358Z","shell.execute_reply":"2024-04-24T22:48:27.595264Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                  question  \\\n0           0                    hi, how are you doing?   \n1           1             i'm fine. how about yourself?   \n2           2       i'm pretty good. thanks for asking.   \n3           3         no problem. so how have you been?   \n4           4          i've been great. what about you?   \n5           5  i've been good. i'm in school right now.   \n6           6                 what school do you go to?   \n7           7                              i go to pcc.   \n8           8                     do you like it there?   \n9           9      it's okay. it's a really big campus.   \n\n                                     answer  \n0             i'm fine. how about yourself?  \n1       i'm pretty good. thanks for asking.  \n2         no problem. so how have you been?  \n3          i've been great. what about you?  \n4  i've been good. i'm in school right now.  \n5                 what school do you go to?  \n6                              i go to pcc.  \n7                     do you like it there?  \n8      it's okay. it's a really big campus.  \n9                    good luck with school.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>hi, how are you doing?</td>\n      <td>i'm fine. how about yourself?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>i'm fine. how about yourself?</td>\n      <td>i'm pretty good. thanks for asking.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>i'm pretty good. thanks for asking.</td>\n      <td>no problem. so how have you been?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>no problem. so how have you been?</td>\n      <td>i've been great. what about you?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>i've been great. what about you?</td>\n      <td>i've been good. i'm in school right now.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>i've been good. i'm in school right now.</td>\n      <td>what school do you go to?</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>what school do you go to?</td>\n      <td>i go to pcc.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>i go to pcc.</td>\n      <td>do you like it there?</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>do you like it there?</td>\n      <td>it's okay. it's a really big campus.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>it's okay. it's a really big campus.</td>\n      <td>good luck with school.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T22:48:27.597619Z","iopub.execute_input":"2024-04-24T22:48:27.597889Z","iopub.status.idle":"2024-04-24T22:48:27.608694Z","shell.execute_reply.started":"2024-04-24T22:48:27.597861Z","shell.execute_reply":"2024-04-24T22:48:27.608125Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0                                           question  \\\n3715        3715                              what's wrong with it?   \n3716        3716                         it aches most of the time.   \n3717        3717                           what do you think it is?   \n3718        3718                i don't know. i think it's old age.   \n3719        3719  if it's old age, why don't both of your hands ...   \n3720        3720    that's a good question. maybe it's not old age.   \n3721        3721                              are you right-handed?   \n3722        3722                                  yes. all my life.   \n3723        3723  you're wearing out your right hand. stop using...   \n3724        3724        but i do all my writing with my right hand.   \n\n                                                 answer  \n3715                         it aches most of the time.  \n3716                           what do you think it is?  \n3717                i don't know. i think it's old age.  \n3718  if it's old age, why don't both of your hands ...  \n3719    that's a good question. maybe it's not old age.  \n3720                              are you right-handed?  \n3721                                  yes. all my life.  \n3722  you're wearing out your right hand. stop using...  \n3723        but i do all my writing with my right hand.  \n3724  start typing instead. that way your left hand ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3715</th>\n      <td>3715</td>\n      <td>what's wrong with it?</td>\n      <td>it aches most of the time.</td>\n    </tr>\n    <tr>\n      <th>3716</th>\n      <td>3716</td>\n      <td>it aches most of the time.</td>\n      <td>what do you think it is?</td>\n    </tr>\n    <tr>\n      <th>3717</th>\n      <td>3717</td>\n      <td>what do you think it is?</td>\n      <td>i don't know. i think it's old age.</td>\n    </tr>\n    <tr>\n      <th>3718</th>\n      <td>3718</td>\n      <td>i don't know. i think it's old age.</td>\n      <td>if it's old age, why don't both of your hands ...</td>\n    </tr>\n    <tr>\n      <th>3719</th>\n      <td>3719</td>\n      <td>if it's old age, why don't both of your hands ...</td>\n      <td>that's a good question. maybe it's not old age.</td>\n    </tr>\n    <tr>\n      <th>3720</th>\n      <td>3720</td>\n      <td>that's a good question. maybe it's not old age.</td>\n      <td>are you right-handed?</td>\n    </tr>\n    <tr>\n      <th>3721</th>\n      <td>3721</td>\n      <td>are you right-handed?</td>\n      <td>yes. all my life.</td>\n    </tr>\n    <tr>\n      <th>3722</th>\n      <td>3722</td>\n      <td>yes. all my life.</td>\n      <td>you're wearing out your right hand. stop using...</td>\n    </tr>\n    <tr>\n      <th>3723</th>\n      <td>3723</td>\n      <td>you're wearing out your right hand. stop using...</td>\n      <td>but i do all my writing with my right hand.</td>\n    </tr>\n    <tr>\n      <th>3724</th>\n      <td>3724</td>\n      <td>but i do all my writing with my right hand.</td>\n      <td>start typing instead. that way your left hand ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T22:48:27.609632Z","iopub.execute_input":"2024-04-24T22:48:27.609908Z","iopub.status.idle":"2024-04-24T22:48:27.623421Z","shell.execute_reply.started":"2024-04-24T22:48:27.609880Z","shell.execute_reply":"2024-04-24T22:48:27.622775Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3725 entries, 0 to 3724\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Unnamed: 0  3725 non-null   int64 \n 1   question    3725 non-null   object\n 2   answer      3725 non-null   object\ndtypes: int64(1), object(2)\nmemory usage: 87.4+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# delete column unnamed\n\ndel data['Unnamed: 0']","metadata":{"execution":{"iopub.status.busy":"2024-04-24T22:48:27.624404Z","iopub.execute_input":"2024-04-24T22:48:27.624681Z","iopub.status.idle":"2024-04-24T22:48:27.633198Z","shell.execute_reply.started":"2024-04-24T22:48:27.624652Z","shell.execute_reply":"2024-04-24T22:48:27.632449Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T22:48:27.634180Z","iopub.execute_input":"2024-04-24T22:48:27.634454Z","iopub.status.idle":"2024-04-24T22:48:27.648202Z","shell.execute_reply.started":"2024-04-24T22:48:27.634426Z","shell.execute_reply":"2024-04-24T22:48:27.647536Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                   question  \\\n0                    hi, how are you doing?   \n1             i'm fine. how about yourself?   \n2       i'm pretty good. thanks for asking.   \n3         no problem. so how have you been?   \n4          i've been great. what about you?   \n5  i've been good. i'm in school right now.   \n6                 what school do you go to?   \n7                              i go to pcc.   \n8                     do you like it there?   \n9      it's okay. it's a really big campus.   \n\n                                     answer  \n0             i'm fine. how about yourself?  \n1       i'm pretty good. thanks for asking.  \n2         no problem. so how have you been?  \n3          i've been great. what about you?  \n4  i've been good. i'm in school right now.  \n5                 what school do you go to?  \n6                              i go to pcc.  \n7                     do you like it there?  \n8      it's okay. it's a really big campus.  \n9                    good luck with school.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hi, how are you doing?</td>\n      <td>i'm fine. how about yourself?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i'm fine. how about yourself?</td>\n      <td>i'm pretty good. thanks for asking.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i'm pretty good. thanks for asking.</td>\n      <td>no problem. so how have you been?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>no problem. so how have you been?</td>\n      <td>i've been great. what about you?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i've been great. what about you?</td>\n      <td>i've been good. i'm in school right now.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>i've been good. i'm in school right now.</td>\n      <td>what school do you go to?</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>what school do you go to?</td>\n      <td>i go to pcc.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>i go to pcc.</td>\n      <td>do you like it there?</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>do you like it there?</td>\n      <td>it's okay. it's a really big campus.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>it's okay. it's a really big campus.</td>\n      <td>good luck with school.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Create text to text generation using model google/flan-t5-base","metadata":{}},{"cell_type":"markdown","source":"# Q1 : hi, how are you doing?","metadata":{}},{"cell_type":"code","source":"!pip install huggingface","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:25:30.737111Z","iopub.execute_input":"2024-04-24T23:25:30.737399Z","iopub.status.idle":"2024-04-24T23:25:34.397278Z","shell.execute_reply.started":"2024-04-24T23:25:30.737370Z","shell.execute_reply":"2024-04-24T23:25:34.396312Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install happytransformer","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:25:34.398934Z","iopub.execute_input":"2024-04-24T23:25:34.399243Z","iopub.status.idle":"2024-04-24T23:25:45.378501Z","shell.execute_reply.started":"2024-04-24T23:25:34.399213Z","shell.execute_reply":"2024-04-24T23:25:45.377616Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting happytransformer\n  Downloading happytransformer-3.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/site-packages (from happytransformer) (3.20.3)\nRequirement already satisfied: tqdm>=4.43 in /usr/local/lib/python3.10/site-packages (from happytransformer) (4.66.2)\nRequirement already satisfied: tokenizers<1.0.0,>=0.13.3 in /usr/local/lib/python3.10/site-packages (from happytransformer) (0.15.2)\nCollecting wandb\n  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/site-packages (from happytransformer) (2.2.1)\nRequirement already satisfied: accelerate<1.0.0,>=0.20.1 in /usr/local/lib/python3.10/site-packages (from happytransformer) (0.29.3)\nCollecting datasets<3.0.0,>=2.13.1\n  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentencepiece\n  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.30.1 in /usr/local/lib/python3.10/site-packages (from happytransformer) (4.39.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (24.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (5.9.8)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (6.0.1)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.4.3)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (1.26.4)\nCollecting pyarrow-hotfix\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2024.3.1)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2.31.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.13.4)\nCollecting multiprocess\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (15.0.2)\nCollecting aiohttp\n  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2.2.2)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.3.8)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (12.1.105)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (2.19.3)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (2.2.0)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (12.1.105)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (1.12)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (8.9.2.26)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (11.0.2.54)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (3.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (4.11.0)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (12.1.0.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (11.4.5.107)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (12.1.3.1)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (10.3.2.106)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (12.1.105)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (12.1.105)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.0->happytransformer) (3.1.3)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0->happytransformer) (12.4.127)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.30.1->happytransformer) (2024.4.16)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb->happytransformer) (65.5.1)\nCollecting appdirs>=1.4.3\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nCollecting docker-pycreds>=0.4.0\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nCollecting sentry-sdk>=1.0.0\n  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/site-packages (from wandb->happytransformer) (8.1.7)\nCollecting setproctitle\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->happytransformer) (1.16.0)\nCollecting async-timeout<5.0,>=4.0\n  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (23.2.0)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (2024.2.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (3.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.0->happytransformer) (2.1.5)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2024.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2024.1)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.0->happytransformer) (1.3.0)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\nInstalling collected packages: sentencepiece, appdirs, xxhash, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, multiprocess, multidict, frozenlist, docker-pycreds, async-timeout, yarl, gitdb, aiosignal, GitPython, aiohttp, wandb, datasets, happytransformer\nSuccessfully installed GitPython-3.1.43 aiohttp-3.9.5 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 datasets-2.19.0 docker-pycreds-0.4.0 frozenlist-1.4.1 gitdb-4.0.11 happytransformer-3.0.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-hotfix-0.6 sentencepiece-0.2.0 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6 xxhash-3.4.1 yarl-1.9.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch transformers tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:32:07.041556Z","iopub.execute_input":"2024-04-24T23:32:07.041966Z","iopub.status.idle":"2024-04-24T23:32:10.686873Z","shell.execute_reply.started":"2024-04-24T23:32:07.041934Z","shell.execute_reply":"2024-04-24T23:32:10.685543Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/site-packages (from torch) (2.2.0)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch) (12.1.3.1)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch) (11.4.5.107)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch) (4.11.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch) (3.13.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch) (12.1.0.106)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch) (10.3.2.106)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/site-packages (from torch) (8.9.2.26)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/site-packages (from torch) (2.19.3)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch) (11.0.2.54)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.4.16)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"hi, how are you doing?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:32:54.447225Z","iopub.execute_input":"2024-04-24T23:32:54.447643Z","iopub.status.idle":"2024-04-24T23:32:56.113322Z","shell.execute_reply.started":"2024-04-24T23:32:54.447611Z","shell.execute_reply":"2024-04-24T23:32:56.112254Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Original Question: hi, how are you doing?\nParaphrases:\n1. How are you feeling?\n2. Hi there, how are you?\n3. What's up, how are you?\n4. Can you tell me how things are going?\n5. How is everything going on?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q2 : i'm fine. how about yourself?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"i'm fine. how about yourself?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:33:46.593499Z","iopub.execute_input":"2024-04-24T23:33:46.593944Z","iopub.status.idle":"2024-04-24T23:33:48.010087Z","shell.execute_reply.started":"2024-04-24T23:33:46.593910Z","shell.execute_reply":"2024-04-24T23:33:48.009223Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Original Question: i'm fine. how about yourself?\nParaphrases:\n1. I'm doing well. How are you?\n2. Hi I'm doing fine. How are you?\n3. \"It's all good. How are you?\"\n4. How do you feel about your relationship?\n5. Is it safe to say that I am in good physical condition?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q3 : no problem. so how have you been?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"no problem. so how have you been?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:34:40.364685Z","iopub.execute_input":"2024-04-24T23:34:40.365106Z","iopub.status.idle":"2024-04-24T23:34:41.627849Z","shell.execute_reply.started":"2024-04-24T23:34:40.365073Z","shell.execute_reply":"2024-04-24T23:34:41.626969Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Original Question: no problem. so how have you been?\nParaphrases:\n1. Can you tell me how your day has been?\n2. I'm sorry, could you tell me how your day has been?\n3. Hi, how's it going?\n4. How are you doing?\n5. What's your current situation?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q4 : i've been good. i'm in school right now.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"i've been good. i'm in school right now.\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:36:13.982976Z","iopub.execute_input":"2024-04-24T23:36:13.983855Z","iopub.status.idle":"2024-04-24T23:36:15.747122Z","shell.execute_reply.started":"2024-04-24T23:36:13.983817Z","shell.execute_reply":"2024-04-24T23:36:15.746226Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Original Question: i've been good. i'm in school right now.\nParaphrases:\n1. My academic performance is impressive, and I'm currently in the same class.\n2. Currently, I'm in school and have been doing well.\n3. It seems like my current situation isn't bad since I'm currently in school.\n4. I've been doing a good job while attending school.\n5. I'm doing well while I attend school right now.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q5 : what school do you go to?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"what school do you go to?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:36:19.992653Z","iopub.execute_input":"2024-04-24T23:36:19.993089Z","iopub.status.idle":"2024-04-24T23:36:21.210946Z","shell.execute_reply.started":"2024-04-24T23:36:19.993034Z","shell.execute_reply":"2024-04-24T23:36:21.210048Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Original Question: what school do you go to?\nParaphrases:\n1. Which educational institution do you attend?\n2. What is the name of your school?\n3. What's the name of the school you attend?\n4. To which educational institution do you attend?\n5. Where do you go to school?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q6 : do you like it there?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"do you like it there?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:37:23.503332Z","iopub.execute_input":"2024-04-24T23:37:23.503741Z","iopub.status.idle":"2024-04-24T23:37:24.813887Z","shell.execute_reply.started":"2024-04-24T23:37:23.503708Z","shell.execute_reply":"2024-04-24T23:37:24.812990Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Original Question: do you like it there?\nParaphrases:\n1. Is there something that appeals to you?\n2. Do you find it enjoyable to be there?\n3. Are you fond of it there?\n4. Does it appeal to you in that place?\n5. Would that be a suitable setting for you?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q7 : i've actually been pretty good. you?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"i've actually been pretty good. you?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:40:22.803827Z","iopub.execute_input":"2024-04-24T23:40:22.804216Z","iopub.status.idle":"2024-04-24T23:40:24.305340Z","shell.execute_reply.started":"2024-04-24T23:40:22.804183Z","shell.execute_reply":"2024-04-24T23:40:24.304428Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Original Question: i've actually been pretty good. you?\nParaphrases:\n1. I'm not complaining, are you okay?\n2. Have you found myself relatively well-suited to my interests and hobbies?\n3. Do you think I'm pretty good?\n4. Can it be said that I'm pretty good at something?\n5. Would you say that I'm pretty good at it?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q8 : so how have you been lately?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"so how have you been lately?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:40:29.033768Z","iopub.execute_input":"2024-04-24T23:40:29.034195Z","iopub.status.idle":"2024-04-24T23:40:30.168304Z","shell.execute_reply.started":"2024-04-24T23:40:29.034159Z","shell.execute_reply":"2024-04-24T23:40:30.167433Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Original Question: so how have you been lately?\nParaphrases:\n1. Can you tell me where you are?\n2. How are you doing?\n3. What's your current state?\n4. How is everything going on with you lately?\n5. Could you share with me your current whereabouts?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q9 : never better, thanks.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"never better, thanks.\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:40:35.033871Z","iopub.execute_input":"2024-04-24T23:40:35.034287Z","iopub.status.idle":"2024-04-24T23:40:36.217975Z","shell.execute_reply.started":"2024-04-24T23:40:35.034252Z","shell.execute_reply":"2024-04-24T23:40:36.217081Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Original Question: never better, thanks.\nParaphrases:\n1. Never in your dreams, thank you.\n2. I'll never experience anything like it, thanks.\n3. Thank you for the opportunity to witness never before.\n4. It has never been better, thank you.\n5. Definitely never more perfect.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q10 : which school do you attend?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Check if GPU is available, if not, use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\n# Define the paraphrase function\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids.to(device)\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    # Decode the generated outputs\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n\n# Test the paraphrase function\nquestion = \"which school do you attend?\"\nparaphrases = paraphrase(question)\nprint(\"Original Question:\", question)\nprint(\"Paraphrases:\")\nfor idx, paraphrase in enumerate(paraphrases):\n    print(f\"{idx+1}. {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:40:41.210037Z","iopub.execute_input":"2024-04-24T23:40:41.210408Z","iopub.status.idle":"2024-04-24T23:40:42.349368Z","shell.execute_reply.started":"2024-04-24T23:40:41.210377Z","shell.execute_reply":"2024-04-24T23:40:42.348459Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Original Question: which school do you attend?\nParaphrases:\n1. What is the school you attend?\n2. Can you tell me which school you are currently studying?\n3. Which educational institution do you belong to?\n4. What's the name of the school you attend?\n5. Where do you go to school?\n","output_type":"stream"}]}]}